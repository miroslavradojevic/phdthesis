% ************************************************************************
% 
% Fuzzy-Logic Based Detection and Characterization of Junctions and Terminations 
% in Fluorescence Microscopy Images of Neurons
%
% ************************************************************************
\chpos{15mm}{8mm}
\chapter[Fuzzy-Logic Based Detection and Characterization of Junctions and Terminations in Fluorescence Microscopy Images of Neurons]{Fuzzy-Logic Based Detection and Characterization of Junctions and Terminations in Fluorescence Microscopy Images of Neurons}
\chaptermark{Fuzzy-Logic Based Detection of Junctions and Terminations}
\label{ch2:fuzzy}
% abstract
{\small \lettrine{D}{igital} reconstruction of neuronal cell morphology is an important step toward understanding the functionality of neuronal networks. Neurons are tree-like structures whose description depends critically on the junctions and terminations, collectively called critical points, making the correct localization and identification of these points a crucial task in the reconstruction process. In this chapter, a fully automatic method for the integrated detection and characterization of both types of critical points in fluorescence microscopy images of neurons is presented. In view of the majority of the current studies (currently available to the authors), which are based on cultured neurons, the method was described and evaluated for application to two-dimensional (2D) images. The method relies on directional filtering and angular profile analysis to extract essential features about the main streamlines at any location in an image, and employs fuzzy logic with carefully designed rules to reason about the feature values in order to make well-informed decisions about the presence of a critical point and its type. Experiments on simulated as well as real images of neurons demonstrate the detection performance of the presented method. A comparison with the output of two existing neuron reconstruction methods reveals that the method described in this chapter achieves substantially higher detection rates and could provide beneficial information to the reconstruction process.\par}
\vspace*{11em}
% ************************************************************************
\begin{publish}
Based upon: M. Radojevi\'{c}, I. Smal, E. Meijering, ``Fuzzy-Logic Based Detection and Characterization of Junctions and Terminations in Fluorescence Microscopy Images of Neurons'', \textit{Neuroinformatics}, vol. 11, no. 11, pp.1-11, 2015.   
\end{publish}

\section{Introduction}
\label{sec:introduction}
The complexity and functionality of the brain depend critically on the morphology and related interconnectivity of its neuronal cells \cite{kandel2000principles, ascoli2002computational, donohue2008comparative}. To understand how a healthy brain processes information and how this capacity is negatively affected by psychiatric and neurodegenerative diseases \cite{anderton1998dendritic, lin2010mechanisms, vsivskova2014dendritic} it is therefore very important to study neuronal cell morphology. Advanced microscopy imaging techniques allow high-sensitivity visualization of individual neurons and produce vast amounts of image data, which are shifting the bottleneck in neuroscience from the imaging to the data processing \cite{svoboda2011past, peng2011proof, senft2011brief, halavi2012digital} and call for a high level of automation. The first processing step toward high-throughput quantitative morphological analysis of neurons is their digital reconstruction from the image data. Many methods have been developed for this in the past decades \cite{meijering2010neuron, donohue2011automated} but the consensus of recent studies is that there is still much room for improvement in both accuracy and computational efficiency \cite{liu2011diadem, svoboda2011past}.

A key aspect of any neuron reconstruction method is the detection and localization of terminations and junctions of the dendritic (and axonal) tree, collectively called ``critical points'' throughout this chapter (Fig.~\ref{ch2_fig1}), which ultimately determine the topology and faithfulness of the resulting digital representation. Roughly there are two ways to extract these critical points in neuron reconstruction \cite{al2008improved, meijering2010neuron, basu2013segmentation}. The most often used approach is to start with segmentation or tracing of the elongated image structures and then to infer the critical points, either afterwards or along the way, by searching for attachments and endings in the resulting subsets \cite{dima2002automatic, xiong2006automated, narro2007neuronmetrics, vasilkoski2009detection, bas2011principal, chothani2011automated, dehmelt2011neuritequant, ho2011neurphologyj, choromanska2012automatic, xiao2013app2}. This approach depends critically on the accuracy of the initial segmentation or tracing procedure, which usually is not designed to reliably capture critical points in the first place and thus often produces very fragmented results, requiring manual postprocessing to fix issues \cite{peng2011proof, luisi2011farsight, dercksen2014filament}. The reverse approach is to first identify critical points in the images and then to use these as seed points for tracing the elongated image structures. Critical points can be obtained either by manual pinpointing, as in semiautomatic tracing methods \cite{meijering2004design, schmitt2004new, narro2007neuronmetrics, lu2009semi, peng2010v3d, longair2011simple}, or by fully automatic detection using sophisticated image filtering and pattern recognition methods (discussed in the chapter sequel). The latter approach has barely been explored for neuron reconstruction, but if reliable detectors can be designed, they provide highly valuable information to the reconstruction process.

\begin{figure}[!b]
	\centering
	\begin{tabular}{c@{\hspace{1em}}c@{\hspace{1em}}}
		\includegraphics[height=0.65\columnwidth]{fig1} &
		\includegraphics[height=0.65\columnwidth]{fig2} \\
		a) & b) 	
	\end{tabular}
	\caption{a) Fluorescence microscopy image of a neuron with manually indicated junctions (red circles) and terminations (yellow circles). The radius of each annotated critical-point region reflects the size of the underlying image structure. b) Example of foreground selection. The original image of $560\times780$ pixels is divided into background (green transparent mask) and foreground (gray-scale regions without mask) \textit{using} $r_{d}=8$ pixels and the $75^\textrm{th}$ variation percentile as threshold. In this example, 25\% of the total number of pixels is selected for further processing.}
	\label{ch2_fig1}
\end{figure}

A novel method is presented - coined Neuron Pinpointer (\gls{np}) - for fully automatic detection and characterisation of critical points in fluorescence microscopy images of neurons. The method is described and evaluated for studies where single (cultured) neurons are imaged in 2D although all aspects of the method can in principle be extended to 3D. The method may also be useful for reconstruction approaches based on 2D projections \cite{zhou2015neuron}. For computational efficiency the method starts with an initial data reduction step, based on local variation analysis, by which obvious background image regions are excluded. In the remaining set of foreground regions the method then explores the local neighborhood of each image pixel and calculates the response to a set of directional filters. Next, an iterative optimization scheme is used for robust peak selection in the resulting angular profile, and a set of corresponding features relevant for the detection task is computed. The feature set is then further processed to make a nonlinear decision on the presence of a critical point and its type (termination or junction) at each foreground image pixel. To conveniently deal with ambiguity and uncertainty in the data, the decision-making is carried out by a fuzzy-logic rule-based system using predefined rules specifically designed for this task. The presented work aims to facilitate the task of automatic neuron reconstruction by contributing a general scheme for extracting critical points that can serve as useful input for any tracing algorithm.

This chapter is a considerably extended version of the conference report \cite{radojevic2014fuzzy}. The filtering algorithms and fuzzy-logic rules had been modified so as to be able to detect both junction and termination points. In addition, the full details of the method are presented here and an extensive evaluation based on both manually annotated real neuron images and computer generated neuron images. To obtain the latter a new computational approach is proposed based on publicly available expert manual tracings. Chapter starts with a brief overview of related work on critical-point detection (Section~\ref{sec:related-work}) and then the underlying concepts (Section~\ref{sec:proposed-method}), implementational details (Section~\ref{sec:implementation-details}), and experimental evaluation (Section~\ref{sec:experiments}) are presented, followed by a summary of the conclusions that can be derived from the results (Section~\ref{sec:conclusions}).

\section{Related Work}
\label{sec:related-work}
Detecting topologically critical points in images has been a long-standing problem in many areas of computer vision. This section provides with a brief review of the problem and proposed solutions in order to put presented work into context.

Examples of previous work include the design of filters to find image locations where either three or more edges join (``junctions of edges'') \cite{sinzinger2008model, hansen2004neural, laganiere2004detection} or three or more lines join (``junctions of lines'') \cite{yu1998rotated, deschenes2000detection}. In biomedical applications, the predominant type of junction is the bifurcation, with occasional trifurcations, as seen in blood vessel trees, bronchial trees, gland ductal trees, and also in dendritic trees \cite{koene2009netmorph, iber2013control}. Hence, research in this area has focused on finding image locations where three (or more) elongated structures join \cite{tsai2004model, agam2005vessel, bevilacqua2005combined, bhuiyan2007automatic, zhou2007vascular, aibinu2010vascular, calvo2011automatic, obaraa2012contrast, su2012junction, azzopardi2013automatic}.

A common approach to find bifurcation points is to infer them from an initial processing step that aims to segment the elongated structures. However, the way these structures are segmented may influence the subsequent critical-point inference. Popular image segmentation methods use intensity thresholding and/or morphological processing, in particular skeletonization \cite{hoover2000locating, dima2002automatic, he2003automated, weaver2004automated, pool2008neuritetracer, bevilacqua2009comparison, leandro2009automatic, aibinu2010vascular}, but these typically produce very fragmented results. Popular methods to enhance elongated image structures prior to segmentation include Hessian based analysis \cite{frangi1998multiscale, xiong2006automated, zhang2007automated, al2008improved, yuan2009mdl, turetken2011automated, myatt2012neuromantic, basu2013segmentation, santamaria2015automatic}, Laplacian-of-Gaussian filters \cite{chothani2011automated}, Gabor filters \cite{bhuiyan2007automatic, azzopardi2013automatic}, phase congruency analysis \cite{obara2012contrast}, and curvelet based image filtering approaches \cite{narayanaswamy20113}. However, being tailored to elongated structures, such filters often yield a less optimal response precisely at the bifurcation points, where the local image structure is more complex than a single ridge.

Several concepts have been proposed to explicitly detect bifurcation points in the images without relying on an initial segmentation of the axonal and dendritic trees. Examples include the usage of circular statistics of phase information \cite{obaraa2012contrast}, steerable wavelet based local symmetry detection \cite{puspoki2013detection}, or combining eigen analysis of the Hessian and correlation matrix \cite{su2012junction}. The problem with existing methods is that they often focus on only one particular type of critical point (for example bifurcations but not terminations), or they use rather rigid geometrical models (for example assuming symmetry), while in practice there are many degrees of freedom \cite{michaelis1994junction}. Image filtering methods for bifurcation detection have also been combined with supervised machine-learning based approaches such as support vector machines \cite{turetken2011automated}, artificial neural networks \cite{bevilacqua2009comparison}, or with multiple classifiers using AdaBoost \cite{zhou2007vascular}, but these lack flexibility in that they require a training stage for each application.

Robust neuron tracing requires knowledge of not only the bifurcation points but also the termination points. Since each type of critical point may vary considerably in terms of geometry (orientation and diameter of the branches) and image intensity (often related to the branch diameter), designing or training a dedicated filter for each possible case is impractical, and a more integrated approach is highly desirable for both detection and characterization of the different types of critical points. To the best of author knowledge, no generic methods currently exist for critical-point detection in neuron images. The method proposed in this chapter aims to fill this gap and to complement exploratory neuron reconstruction algorithms that initialize on a set of seed points.

\section{Proposed Method}
\label{sec:proposed-method}
Proposed method for detection and characterization of critical points consists of three steps: feature extraction (Section~\ref{subsec:feature-extraction}), fuzzy-logic based mapping (Section~\ref{subsec:flrb-detection}), and, finally, critical-point determination (Section~\ref{sec:CPextraction}). Each step is described further in detail. 

\subsection{Feature Extraction}
\label{subsec:feature-extraction}
The aim of the feature extraction step is to compute a set of quantitative features of the local image structure at each pixel position that helps to discriminate between different types of critical points. Since the tree-like neuronal image structures typically cover only a small portion of the image, unnecessary computations are avoided by first performing a foreground selection step (Sec.~\ref{subsubsec:foreground-selection}), which discards image locations that are very unlikely to contain neuronal structures and keeps only those regions that are worthy of further examination. Next, the angular profile (Section \ref{subsubsec:angular-profile}) of each foreground pixel is constructed, from which the quantitative features are computed.

\subsubsection{Foreground Selection}
\label{subsubsec:foreground-selection}
To determine whether a pixel location $(x,y)$ in a given image $I$ should be considered as foreground or background, the local intensity variation $\rho(x,y)$ within a circular neighborhood of radius $r_{d}$ centered at that location is analyzed. To avoid making strong assumptions about the local intensity distribution, the difference between the $95^\textrm{th}$ and the $5^\textrm{th}$ percentile of the intensities within the neighborhood is used as the measure of variation:
\begin{gather} 
\rho(x,y) = \mathcal{P}_{95}(\mathcal{I}_{xy}) - \mathcal{P}_{5}(\mathcal{I}_{xy}) \\
\mathcal{I}_{xy} = \left\{\, I(m,n)\ |\ (m-x)^{2}+(n-y)^{2} \leq r_d^2\, \right\} \\
x,m \in [0,W-1]\ \textrm{and}\ y,n \in [0,H-1]
\end{gather}
where $W$ and $H$ denote, respectively, the width and the height of $I$ in pixels. The value of $\rho$ is relatively low within more or less homogeneous regions (background but also the soma) but relatively high in regions containing neuronal branches. Consequently, the histogram of $\rho$ computed over the entire image contains two clusters (representing foreground and background pixels), which can be separated using simple percentile thresholding \cite{doyle1962operations}. The percentile should be chosen such that background pixels (true negatives) are removed as much as possible while at the same time the foreground pixels (true positives) are retained as much as possible (in practice this implies allowing for false positives). The practical experimentation proved that in this particular application a percentile of around $75$ is a safe threshold (Fig.~\ref{ch2_fig1}b). Small gaps in the foreground region are closed by morphological dilation. The resulting set of foreground pixel locations is denoted by $F$. Similarly, such application requires the parameter $r_{d}$ to be typically set to the diameter of the axonal and dendritic structures observed in the image.

\subsubsection{Angular Profile Analysis}
\label{subsubsec:angular-profile}
For each selected foreground location, a local angular profile is computed and analyzed. The key task here is to assess the presence and properties of any curvilinear image structures passing through the given location. To this end the image is correlated with a set of oriented kernels distributed evenly over a range of angles around that location \cite{radojevic2014fuzzy}. The basic kernel used for this purpose is of size $D\times D$ pixels and has a constant profile in one direction and a Gaussian profile in the orthogonal direction (Fig.~\ref{ch2_fig3}):
\begin{equation}
G(x,y)=\textrm{e}^{-x^2/2\sigma_{\!\!D}^{2}}/S
\label{eq:G}
\end{equation}
where $S$ is a normalization factor such that the sum of $G(x,y)$ over all kernel pixels is unity. The Gaussian is selected both because the cross-sectional profile of axons and dendrites in such applications is observed to be approximately Gaussian-like and because the Gaussian is a theoretically well-justified filter for regularization purposes. The parameters $D$ and $\sigma_{\!D}$ determine the size and shape of the kernel profile and should correspond to the expected branch diameter.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.5\columnwidth]{fig3}
	\caption{Geometry involved in the computation of the angular profile. In effect, the value of $p(x,y,\alpha,k,D)$ is the correlation of the image $I(x,y)$ with the kernel $G(m,n)$ of size $D\times D$ pixels, after rotating the kernel patch over angle $\alpha$ and shifting it over $kD$ with respect to $(x,y)$.}
	\label{ch2_fig3}
\end{figure}

The local angular profile at any pixel location $(x,y)$ in the given image $I$ is computed using the kernel as:
\begin{equation}
p(x,y,\alpha,k,D)=\sum_{m}\sum_{n} I(x_{m,n},y_{m,n})\,G(m,n)
\label{eq:angularprofile}
\end{equation}
where the transformed image coordinates are obtained as:
\begin{equation}
\begin{bmatrix} x_{m,n} \\ y_{m,n} \end{bmatrix} =
\begin{bmatrix} x \\ y \end{bmatrix} +
kD\begin{bmatrix} \sin\alpha \\ -\!\cos\alpha \end{bmatrix} +
\begin{bmatrix} \cos\alpha & -\!\sin\alpha \\ \sin\alpha & \cos\alpha \end{bmatrix}
\begin{bmatrix} m \\ n \end{bmatrix}
\label{eq:xymn}
\end{equation}
and the summation is performed over all $(m,n)$ for which the kernel is defined. That is, $p(x,y,\alpha,k,D)$ is the correlation of the image with the kernel patch rotated over angle $\alpha$ and shifted over a distance $kD$ with respect to $(x,y)$ in the direction corresponding to that angle (Fig.~\ref{ch2_fig3}). In practice, $p$ is calculated for a discrete set of angles, $\alpha_{i}=i/(2\pi N_{\alpha}),\, i=0,\dots,N_{\alpha}-1$, where $N_{\alpha}$ is automatically set such that the circle with radius $kD$ is sampled with pixel resolution. The parameter $k$ is typically set slightly larger than 0.5 so as to scan the neighborhood around the considered pixel $(x,y)$. To obtain the image intensity at non-integer transformed locations $(x_{m,n},y_{m,n})$, linear interpolation is used.

In contrast with previous works, which used differential kernels for directional filtering and profiling \cite{yu1998rotated, can1999rapid, zhang2007automated}, this approach employs the matched kernel (Eq.~\ref{eq:G}), which avoids noise amplification. Although applying a set of rotated kernels is computationally more demanding than Hessian or steerable filtering based methods, it provides more geometrical flexibility in matching the kernels with the structures of interest while retaining excellent directional sensitivity. In presented framework, the computational burden is drastically reduced by the foreground selection step, and further reduction is possible since the filtering process is highly parallelizable.

The computed angular profile is further processed in order to extract several features (Fig.~\ref{ch2_fig4}) relevant for critical-point detection and characterization:

\paragraph{Peaks.} At each foreground pixel location, method determines the number and direction of the line-like image structures which pass through it. This is done by finding the local maxima (``peaks'') in the angular profile at that location. Since the oriented kernels act as low-pass filters, the profile is sufficiently smooth to extract the peaks reliably using the iterative line searching algorithm \cite{flannery1992numerical}. The found peaks correspond to angles $\hat{\alpha}_{i}, i=1,\dots,N_{\hat{\alpha}}$, in which directions the image intensities are the highest. Here $N_{\hat{\alpha}}\leq 4$ to accommodate terminations, normal body points, and junctions (bifurcations and crossovers).

\paragraph{Likelihood.} For each $\hat{\alpha}_{i}$, likelihood $l_i \in \left[ 0, 1 \right]  $ is calculated from the angular profile according to:
\begin{equation} 
l_i = \frac{p(x,y,\hat{\alpha}_{i},k,D)-p_{\min}}{p_{\max}-p_{\min}}
\end{equation}
where $p_{\min}$ and $p_{\max}$ denote, respectively, the minimum and maximum of $p(x,y,\alpha,k,D)$ over $\alpha$.

\paragraph{Energy.} Next, the local grid $\uppi_{i}(x,y,\hat{\alpha}_{i},k,D)$ is considered for each $\hat{\alpha}_{i}$ (Fig.~\ref{ch2_fig4}), consisting of the transformed coordinates $(x_{m,n},y_{m,n})$ corresponding to $\alpha=\hat{\alpha}_{i}$ (Eq.~\ref{eq:xymn}), and a refined centerline point set $\lambda_{i}$ (or ``streamline'') is extracted over this grid by finding for each $n$ the local maximum over $m$:
\begin{gather}
\label{eq:refine-centerline} 
\lambda_{i} = \left\{(x_{\hat{m}_{n},n},y_{\hat{m}_{n},n})\right\}_{n\,\in\,\left[-D/2,D/2\right]}\\[0.5ex]
\hat{m}_{n} = \argmax_{m\,\in\,\left[-D/2,D/2\right]} I(x_{m,n},y_{m,n})
\label{eq:hatm}
\end{gather}
The degree of the streamline deviation from a straight line is quantified by estimating its bending energy $u_{i}\geq0$ as:
\begin{equation}
u_{i} = \frac{1}{\Updelta m}\sum_{n} \left( \hat{m}_{n-1} - 2\hat{m}_{n} + \hat{m}_{n+1} \right)^2
\end{equation}
where $\Updelta m$ is the pixel spacing in the direction of $m$ and the summation extends over all $n$ for which the summand can be evaluated. This calculation is a discrete approximation of the integral squared second-order derivative of the centerline function if it were continuously defined.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.5\columnwidth]{fig4}
	\caption{Flowchart of the feature extraction scheme. The example showcases a bifurcation but the same scheme is used also for terminations. The scheme, which starts with the angular profile $p(x,y,\alpha,k,D)$ and is executed clockwise, is applied to each pixel in the selected foreground regions and results in the set of features $l_i$, $u_i$, and $c_i$, where $i$ indexes the streamlines. See main text for details.}
	\label{ch2_fig4}
\end{figure}

\paragraph{Correlation.} Given a streamline $\lambda_{i}$, the orthogonal direction is estimated at each point in the set by averaging the orthogonal directions of the two neighboring streamline segments corresponding to that point (that is, from the point to the next point, and from the point to the previous point). Using these direction estimates a refined local grid $\Uppi_{i}(x,y,\hat{\alpha}_{i},k,D)$ is sampled, consisting of image coordinates $(\tilde{x}_{m,n},\tilde{y}_{m,n})$ relative to the streamline (Fig.~\ref{ch2_fig4}), and compute a normalized cross-correlation \cite{lewis1995fast} score $c_{i}\in[-1,1]$ as:
\begin{equation} 
\label{eq:correlation}
c_{i} = \frac{\sum_{m}\sum_{n} \bigl[I(\tilde{x}_{m,n},\tilde{y}_{m,n})-\bar{I}\bigr] \bigl[G(m,n)-\bar{G}\bigr]}{\sqrt{\sum_{m}\sum_{n}\bigl[I(\tilde{x}_{m,n},\tilde{y}_{m,n})-\bar{I}\bigr]^{2}\sum_{m}\sum_{n}\bigl[G(m,n)-\bar{G}\bigr]^{2}}}
\end{equation}
where, similar to the angular profile calculation (Eq.~\ref{eq:angularprofile}), the summations extend over all $(m,n)$ for which the kernel is defined, and $\bar{I}$ and $\bar{G}$ denote the mean of the image intensities and of the kernel values, respectively. Effectively $c_{i}$ quantifies the degree to which the template $G$ matches a straightened version of the streamline. To cover a range of possible scales (radii of the underlying image structures), the largest score of a set of templates with standard deviations of the Gaussian profile model \cite{su2012junction} is taken, covering $\left\lbrace 1,\dots,\left\lfloor{\frac{D}{2}}\right\rfloor\right\rbrace $ set of values measured in pixels.

\subsection{Fuzzy-Logic Based Mapping}
\label{subsec:flrb-detection} % https://english.stackexchange.com/questions/242583/rephrase-to-the-best-of-my-knowledge
The feature values extracted at each foreground image location subsequently need to be processed in order to assess the presence of a critical point and its type. Recognizing that in practice everything is ``a matter of degree'' \cite{zadeh1975fuzzy}, and allowing for nonlinear input-output mappings, the method is tailored to use fuzzy logic for this purpose. Fuzzy logic has been successfully used in many areas of engineering \cite{mendel1995fuzzy} but, based on a thorough search of the relevant literature - has not been explored for neuron critical-point analysis. In this chapter, the basics of fuzzy logic (Section~\ref{sec:FL}) are briefly described followed by the description of the fuzzy-logic system tailored for calculating critical-point maps of neuron images (Section~\ref{sec:cp-detection}).
\begin{figure}
	\centering
	\includegraphics[width=0.5\columnwidth]{fig5}
	\caption{Scheme of a single input/output fuzzy-logic (FL) system. A scalar input value $s$ is converted to a vector $\tilde{\mathbf{s}}$ containing the memberships of $s$ for each of the input fuzzy sets, resulting in a vector $\tilde{\mathbf{z}}$ containing the memberships of $z$ for each of the output fuzzy sets.}
	\label{ch2_fig5}
\end{figure}
\subsubsection{Basics of Fuzzy Logic}
\label{sec:FL}
In a fuzzy-logic system (Fig.~\ref{ch2_fig5}), numerical inputs are first expressed in linguistic terms (the fuzzification step), and are then processed based on predefined rules to produce linguistic outputs (the inference step), which are finally turned back into numerical values (the defuzzification step).
\paragraph{Fuzzification.} Given an input scalar value $s\in\mathbb{R}$, the fuzzification step results in a vector $\tilde{\mathbf{s}}$ whose elements express the degree of membership of $s$  to input fuzzy sets, each corresponding to a linguistic term describing $s$. A fuzzy set is defined by a membership function $\mu:\mathbb{R}\rightarrow[0,1]$ quantifying the degree to which $s$ can be described by the corresponding linguistic term. Commonly used membership functions are trapezoidal, Gaussian, triangular, and piecewise linear \cite{mendel1995fuzzy}. As an example, the linguistic terms LOW and HIGH may be used to express the used quantities, representing the subjective notions ``low'' and ``high'', respectively. The degrees to which ``$s$ is low'' (which in this chapter will be denoted as $s=\textrm{LOW}$) and ``$s$ is high'' ($s=\textrm{HIGH}$) are given by membership values $\mu_{\textrm{LOW}}(s)$ and $\mu_{\textrm{HIGH}}(s)$, respectively. The output of the fuzzification step thus becomes $\tilde{\mathbf{s}}=[\mu_{\textrm{LOW}}(s),\mu_{\textrm{HIGH}}(s)]^{T}$.
\paragraph{Inference.} The input fuzzy set memberships are processed by the inference engine to produce a fuzzy output based on rules expressing expert knowledge. The rules can be either explicitly defined or implicitly learned by some training process, and may express nonlinear input-output relationships and involve multiple inputs. In engineering applications, the rules are commonly given as IF-THEN statements about the input and output linguistic terms. For example, the output terms could be OFF, NONE, and ON, indicating whether a certain property of interest is ``off'', ``none'' (expressing ambiguity), or ``on''. A rule could then be:% \begin{split}\end{split} & \\ &
\begin{equation}
R_{i} \! : \ \  \textrm{IF} \  (s_{1}=\textrm{HIGH})\ \wedge\ (s_{2}=\textrm{LOW})\ \ \textrm{THEN}\ \ (z=\textrm{OFF})
\label{eq:rule-prototype}
\end{equation}
where $z\in\mathbb{R}$ is the variable over the output range. This is not a binary logical statement, where the input and output conditions can be only true or false, but a fuzzy logical statement, where the conditions are expressed in terms of memberships, in this case $\mu_{\textrm{HIGH}}(s_{1})$, $\mu_{\textrm{LOW}}(s_{2})$, and $\mu_{\textrm{OFF}}(z)$. Input conditions are often combined using the operators $\wedge$ (denoting fuzzy intersection) or $\vee$ (denoting fuzzy union), which are commonly defined as, respectively, the minimum and maximum of the arguments \cite{mendel1995fuzzy}. In the example, the IF-part of $R_{i}$ (Eq.~\ref{eq:rule-prototype}) would result in the following intermediate value (degree of verity):
\begin{equation}
\upsilon_{i} = \min\left\{\mu_{\textrm{HIGH}}(s_{1}),\mu_{\textrm{LOW}}(s_{2})\right\}
\end{equation}
This value is then used to constrain the fuzzy set corresponding to the output linguistic term addressed by $R_{i}$, in this case OFF, resulting in the output fuzzy set:
\begin{equation}
\Upsilon_{\!i}(z) = \min\left\{\mu_{\textrm{OFF}}(z),\upsilon_{i}\right\}
\end{equation}
In practice there may be many rules $R_{i}, i=1,\dots,N_{R}$, which are aggregated by the inference engine to produce a single output fuzzy set $\Upsilon$. The common way to do this \cite{mendel1995fuzzy} is by means of a weighted fuzzy union:
\begin{equation}
\Upsilon(z) = \max\left\{w_{1}\Upsilon_{\!1}(z),\dots,w_{N_{R}}\Upsilon_{\!N_{R}}(z)\right\}
\end{equation}
Although it is possible to assign a different weight to each rule by setting $w_{i}\in[0,1]$, in the introduced applications this is not critical, and therefore $w_{i}=1$ is used for all $i$.

\paragraph{Defuzzification.} In the final step of the fuzzy-logic system, the fuzzy output $\Upsilon$ is converted back to a scalar output value. Although there are many ways to do this, a common choice is to calculate the centroid \cite{mendel1995fuzzy}:
\begin{equation}
\label{eq:centroid-general}
\hat{z} = \frac{\int z\Upsilon(z)dz}{\int\Upsilon(z)dz} 
\end{equation}
With this value it is possible to finally calculate the vector of output fuzzy set memberships: $\tilde{\mathbf{z}}=[\mu_{\textrm{OFF}}(\hat{z}),\mu_{\textrm{NONE}}(\hat{z}),\mu_{\textrm{ON}}(\hat{z})]^{T}$.

\begin{figure}[!t]
	\centering
	\begin{tabular}{c@{\hspace{1em}}c@{\hspace{1em}}}
	\includegraphics[height=0.27\columnwidth]{fig6a} &
	\includegraphics[height=0.27\columnwidth]{fig6b} \\
	a) & b) 
	\end{tabular}
	\caption{Architecture of the proposed fuzzy-logic system: a) critical-point detection: a cascade of two fuzzy-logic modules ($\textrm{FL}_{1}$ and $\textrm{FL}_{2}$), where the first determines the degree to which streamlines (up to four) are present at the image location under consideration, and based on this information the second determines the degree to which that location corresponds to the possible types of critical points, b) processing the information of one streamline. Input feature values are fuzzified into linguistic terms LOW and HIGH, which are translated by the first fuzzy-logic module ($\textrm{FL}_{1}$) into intermediate linguistic terms OFF, NONE, ON, which are finally translated by the second fuzzy-logic module ($\textrm{FL}_{2}$) into linguistic terms END, NONE, JUN.}
	\label{ch2_fig6}
\end{figure}

\subsubsection{Termination and Junction Mapping}
\label{sec:cp-detection}
To determine the presence and type of critical point at any foreground image location, a cascade of two fuzzy-logic systems is used, representing two decision levels (Fig.~\ref{ch2_fig6}a). The first level takes as input vectors $\mathbf{s}_{i}=[l_{i},u_{i},c_{i}]$, $i=1,\dots,4$, which contain the features for each of the streamlines extracted in the angular profile analysis step at the image location under consideration (Section~\ref{subsubsec:angular-profile}). For each streamline (Fig.~\ref{ch2_fig6}b), the features are fuzzified ($\mu$) and processed by the first fuzzy-logic module ($\textrm{FL}_{1}$), which determines the degree to which the streamline indeed represents a line-like image structure (ON), or not (OFF), or whether the image structure is ambiguous (NONE). In cases where less than four streamlines were found by the angular profile analysis step, the feature vectors of the nonexisting streamlines are set to $\mathbf{0}$. The fuzzy output for all four streamlines together forms the input for the second decision level, where another fuzzy-logic module ($\textrm{FL}_{2}$) determines the degree to which the image location corresponds to a junction (JUN), or a termination (END), or neither of these (NONE).

The input streamline features, $l_{i}$, $u_{i}$, $c_{i}$, are expressed in linguistic terms LOW and HIGH using membership functions $\mu_{\textrm{LOW}}$ and $\mu_{\textrm{HIGH}}$ defined for each type of feature. In the application introduced in this chapter trapezoidal membership functions are used, each having two inflection points, such that $\mu_{\textrm{LOW}}$ and $\mu_{\textrm{HIGH}}$ are each other's complement (Fig.~\ref{ch2_fig7}). For example, the degrees to which $l_{i}=\textrm{LOW}$ and $l_{i}=\textrm{HIGH}$, are given by $l_{i}^{\textrm{LOW}}=\mu^{L}_{\textrm{LOW}}(l_{i})$ and $l_{i}^{\textrm{HIGH}}=\mu^{L}_{\textrm{HIGH}}(l_{i})=1-l_{i}^{\textrm{LOW}}$, respectively, and because of this complementarity it is sufficient to use $\mu^{L}$  to refer to both membership functions (Fig.~\ref{ch2_fig6}) throughout the manuscript. Similarly, the membership degrees of $u_{i}$ and $c_{i}$ are given by $\mu^{U}$ and $\mu^{C}$, respectively. In summary, the following notations and definitions for the fuzzification step are used:
\begin{equation}
\begin{array}{lcccl}
\mu^{L}\!:\ & l_{i} & \rightarrow & \tilde{\mathbf{l}}_{i} & = \left[l_{i}^{\textrm{LOW}}\!,\, l_{i}^{\textrm{HIGH}}\right]^T \\[1ex]
\mu^{U}\!:\ & u_{i} & \rightarrow & \tilde{\mathbf{u}}_{i} & = \left[u_{i}^{\textrm{LOW}}\!,\, u_{i}^{\textrm{HIGH}}\right]^T \\[1ex]
\mu^{C}\!:\ & c_{i} & \rightarrow & \tilde{\mathbf{c}}_{i} & = \left[c_{i}^{\textrm{LOW}}\!,\, c_{i}^{\textrm{HIGH}}\right]^T
\end{array}
\end{equation}
and the lower and higher inflection points of $\mu^{L}$ are denoted by $L_{\textrm{LOW}}$ and $L_{\textrm{HIGH}}$, and similarly $U_{\textrm{LOW}}$ and $U_{\textrm{HIGH}}$ for $\mu^{U}$, and $C_{\textrm{LOW}}$ and $C_{\textrm{HIGH}}$ for $\mu^{C}$ (Fig.~\ref{ch2_fig7}).

\begin{figure}%[!t]
	\centering
	\includegraphics[height=9em]{L_fuzzification_1}
	\includegraphics[height=9em]{L_fuzzification_2} \\[2ex]
	\includegraphics[height=9em]{U_fuzzification_1}
	\includegraphics[height=9em]{U_fuzzification_2} \\[2ex]
	\includegraphics[height=9em]{C_fuzzification_1}
	\includegraphics[height=9em]{C_fuzzification_2} \\
	\caption{Input membership functions used in the fuzzification step for $\textrm{FL}_{1}$. Example LOW and HIGH membership values are shown (right column) for input values (dashed vertical lines in the plots on the left) $l_{i}=0.35$ (top row), $u_{i}=10$ (middle row), and $c_{i}=0.85$ (bottom row). The inflection points of the membership functions are, respectively, $L_{\textrm{LOW}}=0.05$ and $L_{\textrm{HIGH}}=0.4$ for $\mu^{L}$, $U_{\textrm{HIGH}}=5$ and $U_{\textrm{LOW}}=20$ for $\mu^{U}$, and $C_{\textrm{LOW}}=0.5$ and $C_{\textrm{HIGH}}=0.95$ for $\mu^{C}$. Notice that features $u_{i}$ (the centerline bending energies of the streamlines) are reinterpreted here to express the degree of smoothness (hence the inverted membership functions as compared to the other two).}
	\label{ch2_fig7}
\end{figure}
Taken together, the input to $\textrm{FL}_{1}$ is the matrix of memberships $\tilde{\mathbf{s}}_{i}=[\tilde{\mathbf{l}}_{i},\tilde{\mathbf{u}}_{i},\tilde{\mathbf{c}}_{i}]$, and the output is the vector $\tilde{\mathbf{o}}_{i}$ of memberships to the linguistic terms OFF, NONE, ON:
\begin{equation}
\mathrm{FL}_{1}\!:\ \tilde{\mathbf{s}}_{i} \rightarrow \tilde{\mathbf{o}}_{i} = \left[o_{i}^{\textrm{OFF}}\!,\, o_{i}^{\textrm{NONE}}\!,\, o_{i}^{\textrm{ON}}\right]^T
\label{eq:FL1}
\end{equation}
To calculate these memberships, scalar variable $o$ is introduced, where $o=0$ corresponds to OFF, $o=1$ to NONE, and $o=2$ to ON. Also, Gaussian membership functions $\mu_{\textrm{OFF}}^{O}$, $\mu_{\textrm{NONE}}^{O}$ and $\mu_{\textrm{ON}}^{O}$  are defined, centered around $0$, $1$, and $2$, respectively (Fig.~\ref{ch2_fig9}), and with fixed standard deviation $0.4$ so that they sum to about 1 in the interval $[0,2]$. The rules used by $\textrm{FL}_{1}$ to associate the input terms LOW and HIGH to the output terms OFF, NONE, and ON, are given in Table~\ref{tab:rules-FL1}. They are based on the heuristic assumption that a line-like image structure exists (ON) if the evidence represented by all three features support it (HIGH). By contrast, if the likelihood is LOW and at least one other feature is also LOW, this indicates that no such structure exists (OFF). In all remaining cases, some structure may exist, but it is not line-like (NONE). As an example, rule $R_8$ (Table~\ref{tab:rules-FL1}) is given by:
\begin{equation}
R_8\!:\ \ \textrm{IF}\ (l=\textrm{HIGH})\ \wedge\ (u=\textrm{HIGH})\ \wedge\ (c=\textrm{HIGH}) \textrm{THEN}\ (o = \textrm{ON})
\end{equation}
which results in the verity value:
\begin{equation}
\upsilon_{8} = \min\left\{\mu_{\textrm{HIGH}}^{L}(l), \mu_{\textrm{HIGH}}^{U}(u), \mu_{\textrm{HIGH}}^{C}(c)\right\}
\end{equation}
and the output fuzzy set: 
\begin{equation}
\Upsilon_{8}(o) = \min\left\{\mu_{\textrm{ON}}^{O}(o),\upsilon_{8}\right\}
\end{equation}
All the rules are resolved and combined as:
\begin{equation}
\Upsilon(o) = \max\left\{\Upsilon_{1}(o),\dots,\Upsilon_{8}(o)\right\}
\end{equation}
and centroid defuzzification then results in a scalar output value $\hat{o}$. This procedure is repeated for each streamline, yielding $\hat{o}_{i}, i=1,\dots,4$, from which the output of each $\mathrm{FL}_{1}$ (Eq.~\ref{eq:FL1}) is calculated using the membership functions:
\begin{equation}
\tilde{\mathbf{o}}_{i} = \left[\mu_{\textrm{OFF}}^{O}(\hat{o}_{i}),\, \mu_{\textrm{NONE}}^{O}(\hat{o}_{i}),\, \mu_{\textrm{ON}}^{O}(\hat{o}_{i})\right]^{T}
\end{equation}

\begin{figure}
	\centering
	\includegraphics[height=9em]{fig9a}
	\includegraphics[height=9em]{fig9b}
	\caption{Output membership functions used in module $\mathrm{FL}_{1}$. Example output fuzzy sets $\Upsilon_{\!i}$ corresponding to rules $R_i$ from Table~\ref{tab:rules-FL1} are shown as the textured areas. Value $\hat{o}$ (left panel) represents the centroid of the aggregated output fuzzy sets. The resulting output membership values (right panel) serve as input for module $\mathrm{FL}_{2}$.}
	\label{ch2_fig9}
\end{figure}

\begin{table}[!t]
	\caption{The set of rules employed by $\mathrm{FL}_{1}$.}
	\label{tab:rules-FL1}
	\centering
	\begin{tabular}{c | c c c | c }
		$R_{i}$ & $l$ & $u$ & $c$ & $o$ \\
		\hline
		1 & LOW  & LOW  & LOW  & OFF \\
		2 & LOW  & LOW  & HIGH & OFF \\
		3 & LOW  & HIGH & LOW  & OFF \\
		\hline
		4 & LOW  & HIGH & HIGH  & NONE \\ 
		5 & HIGH & LOW  & LOW   & NONE \\ 
		6 & HIGH & LOW  & HIGH  & NONE \\ 
		7 & HIGH & HIGH & LOW   & NONE \\ 
		\hline
		8 & HIGH & HIGH & HIGH  & ON \\ 
	\end{tabular}
\end{table}

\begin{table}[!t]
	\caption{The set of rules employed by $\mathrm{FL}_{2}$. Empty entries indicate ``don't care'' (could be OFF, NONE, or ON).}
	\label{tab:rules-FL2}
	\centering
	\begin{tabular}{c | c c c c | c }
		$R_{i}$ & $o_{1}$   & $o_{2}$   & $o_{3}$   & $o_{4}$  & $z$ \\
		\hline
		1 & OFF  & OFF  & OFF  & OFF & NONE \\
		2 & OFF  & OFF  & OFF  & ON  & END  \\
		3 & OFF  & OFF  & ON   & OFF & END  \\
		4 & OFF  & OFF  & ON   & ON  & NONE \\
		\hline
		5 & OFF  & ON   & OFF  & OFF & END  \\
		6 & OFF  & ON   & OFF  & ON  & NONE \\
		7 & OFF  & ON   & ON   & OFF & NONE \\
		8 & OFF  & ON   & ON   & ON  & JUN  \\
		\hline
		9 & ON   & OFF  & OFF  & OFF & END  \\
		10&ON   & OFF  & OFF  & ON  & NONE  \\
		11&ON   & OFF  & ON   & OFF & NONE  \\
		12&ON   & OFF  & ON   & ON  & JUN  \\
		\hline
		13&ON   & ON   & OFF  & OFF & NONE  \\
		14&ON   & ON   & OFF  & ON  & JUN  \\
		15&ON   & ON   & ON   & OFF & JUN  \\
		16&ON   & ON   & ON   & ON  & JUN  \\
		\hline
		17&NONE & NONE &      &      & NONE \\
		18&NONE &      & NONE &      & NONE \\
		19&NONE &      &      & NONE & NONE \\
		20&     & NONE & NONE &      & NONE \\
		21&     & NONE &      & NONE & NONE \\
		22&     &      & NONE & NONE & NONE \\     
	\end{tabular}
\end{table}

Moving on to the next level, the input to $\mathrm{FL}_{2}$ is the matrix of memberships $\tilde{\mathbf{o}}=\left[\tilde{\mathbf{o}}_{1},\tilde{\mathbf{o}}_{2},\tilde{\mathbf{o}}_{3},\tilde{\mathbf{o}}_{4}\right]$, and the output is the vector $\tilde{\mathbf{z}}$ of memberships to the linguistic terms END (termination), NONE (no critical point), JUN (junction):
\begin{equation}
\mathrm{FL}_{2}\!:\ \tilde{\mathbf{o}} \rightarrow \tilde{\mathbf{z}} = \left[z^{\textrm{END}}\!,\, z^{\textrm{NONE}}\!,\, z^{\textrm{JUN}}\right]^T
\label{eq:FL2}
\end{equation}
To calculate these memberships, scalar variable $z$ is introduced, where $z=1$ corresponds to END, $z=2$ to NONE, and $z=3$ to JUN. Corresponding Gaussian membership functions $\mu_{\textrm{END}}^{Z}$, $\mu_{\textrm{NONE}}^{Z}$, and $\mu_{\textrm{JUN}}^{Z}$ are defined. They are centered around $1$, $2$, and $3$, respectively, and with fixed standard deviation $0.4$ as before (Fig.~\ref{ch2_fig10}). The rules used by $\mathrm{FL}_{2}$ to associate the input terms OFF, NONE, ON to the output terms END, NONE, JUN are given in Table~\ref{tab:rules-FL2}. They are based on the heuristic assumption that there is a termination (END) if a single streamline is confirmed to correspond to a line-like image structure (ON) and the other three are confirmed to not correspond to such a structure (OFF). Conversely, if at least three are ON, there must be a junction at that location. Finally, if two are ON and two are OFF, or if at least two streamlines are ambiguous (NONE), it is assumed that there is no critical point. Similar to $\mathrm{FL}_{1}$, all the rules of $\mathrm{FL}_{2}$ are evaluated and their results combined as:
\begin{equation}
\Upsilon(z) = \max\left\{\Upsilon_{1}(z),\dots,\Upsilon_{22}(z)\right\}
\end{equation}
which, after centroid defuzzification, results in a scalar output value $\hat{z}$, from which the output of $\mathrm{FL}_{2}$ (Eq.~\ref{eq:FL2}) is calculated using the membership functions:
\begin{equation}
\tilde{\mathbf{z}} = \left[\mu_{\textrm{END}}^{Z}(\hat{z}),\, \mu_{\textrm{NONE}}^{Z}(\hat{z}),\, \mu_{\textrm{JUN}}^{Z}(\hat{z})\right]^{T}
\end{equation}
The proposed fuzzy-logic system is applied to each foreground pixel location $(x,y)\in F$ (Section~\ref{subsubsec:foreground-selection}) so that all memberships introduced above may be indexed by $(x,y)$. Based on this, the following two maps are calculated:
\begin{equation}
I_{\textrm{END}}(x,y)=\left\{
\begin{array}{l@{\qquad}l}
z^{\textrm{END}}(x,y) & \textrm{if}\ (x,y)\in F \\
0 & \textrm{otherwise}
\end{array}
\right.
\end{equation}
\begin{equation}
I_{\textrm{JUN}}(x,y)=\left\{
\begin{array}{l@{\qquad}l}
z^{\textrm{JUN}}(x,y) & \textrm{if}\ (x,y)\in F \\
0 & \textrm{otherwise}
\end{array}
\right.
\end{equation}
which indicate the degree to which any pixel $(x,y)$ belongs to a termination or a junction, respectively.
\begin{figure}[ht!]
	\centering
	\includegraphics[height=9em]{fig10a}
	\includegraphics[height=9em]{fig10b}
	\caption{Output membership functions used in module $\mathrm{FL}_{2}$. Example output fuzzy sets $\Upsilon_{\!i}$ corresponding to rules $R_i$ from Table~\ref{tab:rules-FL2} are shown as the textured areas. Value $\hat{z}$ (left panel) represents the centroid of the aggregated output fuzzy sets. The resulting output membership values (right panel) indicate the degree to which there may be a termination (END), junction (JUN), or neither of these (NONE) at the image pixel location under consideration.}
	\label{ch2_fig10}
\end{figure}
\subsection{Critical-Point Determination}
\label{sec:CPextraction}
The ultimate aim of the introduced method is to provide a list of critical points in the neuron image, with each point fully characterized in terms of type, location, size, and main branch direction(s). Since each critical point of a neuronal tree typically covers multiple neighboring pixels in the image, giving rise to a high value at the corresponding pixels in the maps $I_{\textrm{END}}$ and $I_{\textrm{JUN}}$, the final task is to segment the maps and to aggregate the information within each segmented region.
\begin{figure}[!b]
	\centering
	\begin{tabular}{c@{\hspace{2em}}c@{\hspace{2em}}}
	\includegraphics[height=9em]{cp-region} &
	\includegraphics[height=9em]{cp-region-mapping}
	\end{tabular}
	\caption{Critical-point determination. A critical point is characterized by its type, centroid location $(x_C,y_C)$, radius $r_C$, and its main branch directions $\hat{\mathbf{v}}_i$ (left panel, in this case a bifurcation), aggregated from the pixels $(x_p,y_p)$ in the corresponding critical region (right panel).}
	\label{ch2_fig11}
\end{figure}
Due to noise, labeling imperfections, and structural ambiguities in the original image, the values of neighboring pixels in the maps may vary considerably, and direct thresholding usually does not give satisfactory results. To improve the robustness the real-valued scores in the maps are first regularized by means of local-average filtering with a radius of 3-5 pixels. Next, max-entropy based automatic thresholding \cite{kapur1985new} is applied to segment the maps, as in contrast with many other thresholding methods it was found to perform well in separating the large but relatively flat (low information) background regions from the much smaller but more fluctuating (high information) regions of interest. The resulting binary images are further processed using a standard connected components algorithm \cite{sonka2014image} to identify the critical-point regions.

Each critical region consists of a set of connected pixels $\mathbf{x}_p=(x_p,y_p)$, $p=1,\dots,N_p$, where $N_p$ denotes the number of pixels in the region. From these, the representative critical-point location $\mathbf{x}_C=(x_C,y_C)$ is calculated as:
\begin{equation}
\mathbf{x}_C=\frac{1}{N_p}\sum_{p=1}^{N_p}\mathbf{x}_p
\end{equation}
while the critical-point size is represented by the radius of the minimum circle surrounding the region:
\begin{equation}
r_C = \max_{p}\left\{||\mathbf{w}_p||\right\}
\end{equation}
where $\mathbf{w}_p=\mathbf{x}_p-\mathbf{x}_C$ (Fig.~\ref{ch2_fig11}). To obtain regularized estimates of the main branch directions $\hat{\mathbf{v}}_i$ for the critical point, the directions are aggregated corresponding to the angular profile peaks $\hat{\alpha}_i$ (Section~\ref{subsubsec:angular-profile}) of all the $\mathbf{x}_p$ in the region as follows. For each $\mathbf{x}_p$, $N_{\hat{\alpha}}\leq4$ angular profile peak direction vectors $\mathbf{a}_{p,i}=[\cos\hat{\alpha}_i(\mathbf{x}_p),\sin\hat{\alpha}_i(\mathbf{x}_p)]^T$ are found. Each of these vectors defines a line $\mathbf{a}(t)=\mathbf{x}_{p}+t\,\mathbf{a}_{p,i}$ parameterized by $t\in\mathbb{R}$. The projection of this line onto the circle $||\mathbf{x}-\mathbf{x}_C||^2=r^2_C$ is established by substituting $\mathbf{x}=\mathbf{a}(t)$ and solving for $t$. From this, the contributing unit vector is calculated (Fig.~\ref{ch2_fig11}):
\begin{equation}
\mathbf{e}_{p,i}=\frac{1}{r_C}(\mathbf{w}_{p}+t\,\mathbf{a}_{p,i})
\end{equation}
which points from $\mathbf{x}_C$ to the intersection point. This is done for all $p=1,\dots,N_p$ in the region and $i=1,\dots,N_{\hat{\alpha}}$ for each $p$, resulting in the set of vectors $\{\mathbf{e}_{p,i}\}$. Next, a recursive mean-shift clustering algorithm \cite{cheng1995mean} is applied to $\{\mathbf{e}_{p,i}\}$, which converges to a set $\{\hat{\mathbf{v}}_{i}\}$, where the cluster vectors $\hat{\mathbf{v}}_{i}$, $i=1,\dots,L$, represent the branches. For a critical region in $I_{\textrm{END}}$, only one main branch direction is needed, simply taken to be the direction $\hat{\mathbf{v}}_1$ to which the largest number of $\mathbf{e}_{p,i}$ were shifted. For a critical region in $I_{\textrm{JUN}}$, the $\hat{\mathbf{v}}_i$ (at least three) are taken as the main branch directions to which the largest number of $\mathbf{e}_{p,i}$ were shifted. These calculations are performed for all critical regions.

\section{Implementational Details}
\label{sec:implementation-details}
The method was implemented in the Java programming language as a plugin for the image processing and analysis tool ImageJ \cite{abramoff2004image, schneider2012nih}. Since the feature extraction step (Section~\ref{subsec:feature-extraction}), in particular the matched filtering for angular profile analysis, is quite computationally demanding, parallelization is applied in multiple ways to reduce the running time to acceptable levels (on the order of minutes on a regular PC). Specifically, the directional filtering was split between CPU cores, each taking care of a subset of the directions (depending on the number of available cores). After this, the angular profile analysis and calculation of the features was also split, with each core processing a subset of the foreground image locations. This was sufficient to run the experiments. Further improvement in running time (down to real-time if needed) could be achieved by mass parallelization using GPUs (graphical processing units) instead of CPUs.

Essential parameters that need to be set by the user are the scale parameters $k$ and $D$ (Section~\ref{subsubsec:angular-profile}) and the inflection points $L_{\mathrm{LOW}}$, $L_{\mathrm{HIGH}}$, $U_{\mathrm{LOW}}$, $U_{\mathrm{HIGH}}$, $C_{\mathrm{LOW}}$, and $C_{\mathrm{HIGH}}$ of the input membership functions used by fuzzy-logic module $\textrm{FL}_{1}$ (Section~\ref{sec:cp-detection}). In the showcased application, parameter $D$ is set to the expected neuron diameter in a given set of images while $k=0.7$ was kept fixed. The $L$ inflection points are always in the range $[0,1]$ since the corresponding feature (likelihood) is normalized. Based on ample experience with many data sets, $L_{\mathrm{LOW}}$ is typically set close to 0 and $L_{\mathrm{HIGH}}$ around $0.5$ (Fig.~\ref{ch2_fig7}). By contrast, the inflection points $U$ correspond to a feature (centerline bending energy) that is not normalized and may vary widely from $0$ to any positive value. To obtain sensible values for these, the histogram of all calculated energy values in the image is used. Parameter $U_{\mathrm{LOW}}$ is set to the threshold computed by the well-known triangle algorithm, while typically $U_{\mathrm{HIGH}}\gg U_{\mathrm{LOW}}$. It is useful to note that the membership functions defined by these parameters are inverted (Fig.~\ref{ch2_fig7}) such that the energy becomes a measure of smoothness. Finally, the $C$ inflection points correspond again to a feature (correlation) with a fixed output range $[-1,1]$. In practical applications they are usually set to $C_{\mathrm{LOW}}\in[0.1,0.5]$ and $C_{\mathrm{HIGH}}=0.95$  (Fig.~\ref{ch2_fig7}).

All other aspects of the method that could be considered as user parameters either follow directly from these essential parameters or are fixed to the standard values mentioned in the text. For example, the radius $r_d$ of the circular neighborhood in the foreground selection step (Section~\ref{subsubsec:foreground-selection}) can be set equal to $D$, and the standard deviation $\sigma_{\!D}$ of the Gaussian profile (Section~\ref{subsubsec:angular-profile}) can be set to $D/6$ to get a representative shape. Also, the output membership functions of $\textrm{FL}_1$ (input to $\textrm{FL}_2$) as well as the output membership functions of $\textrm{FL}_2$ are Gaussians with fixed levels and standard deviation (Section~\ref{sec:cp-detection}), as they are not essentially influencing the performance of the algorithm.
\begin{figure}%[t!]
	\centering
	\includegraphics[width=\columnwidth]{fig12}
	\caption{Examples of simulated triplet images and detection results. Each triplet consists of three branches with different diameters which join at one end to form a bifurcation point and with the other ends being termination points. Images were generated at $\textrm{SNR}=2,3,4,5$ (left to right panel). The detection results with presented method are indicated as red circles (bifurcation points) and yellow circles (termination points), where the radius of each circle reflects the size of the critical region found by the method.}
	\label{ch2_fig12}
\end{figure}
\section{Experimental Results}
\label{sec:experiments}
To evaluate the performance of Neuron Pinpointer method in correctly detecting and classifying neuronal critical points, experiments with simulated images (using the ground truth available from the simulation) as well as with real fluorescence microscopy images (using manual annotation as the gold standard) have been performed. After outlining the performance measures (Section \ref{subsec:performance-measures}), the results of the evaluation on simulated images are presented, including the synthetic triplets (Section \ref{subsec:experiments-triplets}) and synthetic neurons (Section \ref{subsec:experiments-simulated}), and on real neuron images (Section \ref{subsec:experiments-real}). Finally, the results of a comparison of the method with two other methods (Section \ref{subsec:comparison}) is showcased.
\subsection{Performance Measures}
\label{subsec:performance-measures}
Performance was quantified by counting the correct and incorrect hits and the misses of the detection with respect to the reference data. More specifically, the true-positive (TP), false-positive (FP), and the false-negative (FN) critical-point detections had been counted, and used to calculate the recall $\textrm{R}=\textrm{TP}/(\textrm{TP}+\textrm{FN})$ and precision $\textrm{P}=\textrm{TP}/(\textrm{TP}+\textrm{FP})$. Both R and P take on values in the range from $0$ (meaning total failure) to $1$ (meaning flawless detection). They are commonly combined in the F-measure \cite{powers2011evaluation}, defined as the harmonic mean of the two: $\textrm{F}=2\,\textrm{R}\,\textrm{P}/(\textrm{R}+\textrm{P})$. The F-measure was computed separately for each type of critical points considered in this paper, yielding $\textrm{F}_{\textrm{END}}$ for terminations and $\textrm{F}_{\textrm{JUN}}$ for junctions. As a measure of overall performance, the harmonic mean of the two F-measures: $\textrm{F}_{\textrm{BOTH}}=2\,\textrm{F}_{\textrm{END}}\,\textrm{F}_{\textrm{JUN}}/(\textrm{F}_{\textrm{END}}+\textrm{F}_{\textrm{JUN}})$ is also computed.
\begin{figure}
	\centering
	\begin{tabular}{c@{\hspace{1em}}c@{\hspace{1em}}}
	\includegraphics[width=0.4\columnwidth]{triplets_fjun_fend_vs_pratio} &
	\includegraphics[width=0.4\columnwidth]{triplets_fboth_vs_snr}
	\end{tabular}
	\caption{Performance of the method in detecting terminations and junctions in simulated images of triplets. The values of $\textrm{F}_{\textrm{END}}$ and $\textrm{F}_{\textrm{JUN}}$ are shown (left panel) for the various branch diameter ratios $d_{\max}/d_{\min}$ at $\textrm{SNR} = 4$. The distribution of $\textrm{F}_{\textrm{BOTH}}$ values is shown as a box plot (right panel) for the various SNR levels.}
	\label{ch2_fig13}
\end{figure}

\subsection{Evaluation on Simulated Triplet Images}
\label{subsec:experiments-triplets}
Before considering full neuron imagery the performance of the method was first evaluated at detecting terminations and junctions in a very basic configuration as a function of image quality. To this end, a triplet model was used, consisting of a single junction modeling a bifurcation, having three branches with arbitrary orientations (angular intervals) and diameters (Fig.~\ref{ch2_fig12}). Orientations were randomly sampled from a uniform distribution in the range $[0, 2\pi]$ while prohibiting branch overlap. Since in principle the directional filtering step (Section \ref{subsubsec:angular-profile}) uses a fixed kernel size $D$, the intent was to investigate the detection robustness for varying ratios $d_{\max}/d_{\min}$ between the maximum and the minimum branch diameter in a triplet. For such experiment, $1,0.33,2,2.5,3$ ratios were considered by taking normalized diameter configurations ($d_1,d_2,d_3$) = (0.33,0.33,0.33), (0.3,0.3, 0.4), (0.2, 0.4,0.4), (0.2,0.3,0.5), (0.2,0.2,0.6), where in each case the actual smallest diameter was set to 3 pixels (the resolution limit) and the other diameters were scaled accordingly. A set of images was simulated for each configuration containing $1,000$ well-separated triplets for signal-to-noise ratio levels $\textrm{SNR}=2$, $3$, $4$, $5$ (see cropped examples in Fig.~\ref{ch2_fig12}). These levels are chosen knowing that $\textrm{SNR}=4$ is a critical level in other detection problems \cite{smal2010quantitative, chenouard2014objective}. Poisson noise was used in simulating fluorescence microscopy imaging of the triplets. The obtained results of this experiment (Fig.~\ref{ch2_fig13}) lead to the conclusion that the method is fairly robust for diameter ratios $d_{\max}/d_{\min}\leq2\frac{1}{2}$ and an image quality of $\textrm{SNR} \geq 4$. Based on the detection rates, it is also possible to draw a conclusuion that the presented method is somewhat better in detecting terminations than detecting junctions in synthetic images. Example detection results for $d_{\max}/d_{\min}\leq2$ for the considered SNR levels are shown in Fig.~\ref{ch2_fig12}.

\begin{figure}
	\centering
	\begin{tabular}{c@{\hspace{0.1em}}c@{\hspace{0.1em}}c@{\hspace{0.1em}}c@{\hspace{0.1em}}}
		\includegraphics[width=0.2\columnwidth]{dekoninck-snr-2} &
		\includegraphics[width=0.2\columnwidth]{dekoninck-snr-3} &
		\includegraphics[width=0.2\columnwidth]{dekoninck-snr-4} &
		\includegraphics[width=0.2\columnwidth]{dekoninck-snr-5}\\%[0.1ex]
		\includegraphics[width=0.2\columnwidth]{strettoi-snr-2} &
		\includegraphics[width=0.2\columnwidth]{strettoi-snr-3} &
		\includegraphics[width=0.2\columnwidth]{strettoi-snr-4} &
		\includegraphics[width=0.2\columnwidth]{strettoi-snr-5}\\%[0.1ex]
		\includegraphics[width=0.2\columnwidth]{masland-snr-2} &
		\includegraphics[width=0.2\columnwidth]{masland-snr-3} &
		\includegraphics[width=0.2\columnwidth]{masland-snr-4} &
		\includegraphics[width=0.2\columnwidth]{masland-snr-5}\\%[0.1ex]
		\includegraphics[width=0.2\columnwidth]{burdakov-snr-2} &
		\includegraphics[width=0.2\columnwidth]{burdakov-snr-3} &
		\includegraphics[width=0.2\columnwidth]{burdakov-snr-4} &
		\includegraphics[width=0.2\columnwidth]{burdakov-snr-5}\\%[0.1ex]
		\includegraphics[width=0.2\columnwidth]{jacobs-snr-2} &
		\includegraphics[width=0.2\columnwidth]{jacobs-snr-3} &
		\includegraphics[width=0.2\columnwidth]{jacobs-snr-4} &
		\includegraphics[width=0.2\columnwidth]{jacobs-snr-5}
	\end{tabular}
	\caption{Examples of simulated neuron images based on expert reconstructions from the NeuroMorpho.Org database. The images show a wide range of morphologies (one type per row) and image qualities of $\textrm{SNR}=2$, $3$, $4$, $5$ (from left to right per row).}
	\label{ch2_fig14}
\end{figure}

\begin{figure}
	\centering
	\begin{tabular}{c@{\hspace{1em}}c@{\hspace{1em}}}
		\includegraphics[width=0.4\columnwidth]{nmorpho_snr4} &
		\includegraphics[width=0.4\columnwidth]{nmorpho_snr}
	\end{tabular}
	\caption{Performance of the method in detecting terminations and junctions in 30 simulated images of neurons. The distributions of the $\textrm{F}_\textrm{END}$, $\textrm{F}_\textrm{JUN}$, and $\textrm{F}_\textrm{BOTH}$ values are shown as box plots for $\textrm{SNR}=4$ (left panel) and in addition the distribution of $\textrm{F}_\textrm{BOTH}$ is shown for $\textrm{SNR}=2$, $3$, $4$, $5$ (right panel).}
	\label{ch2_fig15}
\end{figure}

\subsection{Evaluation on Simulated Neuron Images}
\label{subsec:experiments-simulated}
To evaluate the method on more complex images, with the known ground truth, the imaging of complete neurons was simulated. Although there are various ways this could be done \cite{koene2009netmorph, vasilkoski2009detection}, the existing expert reconstructions from the NeuroMorpho.Org database \cite{ascoli2007neuromorpho} were used. A total of 30 reconstructions from different neuron types were downloaded as SWC files \cite{cannon1998line}, in which the reconstructions are represented as a sequence of connected center-point locations in 3D with corresponding radii in micrometers. Fluorescence microscopy images were generated from these reconstructions in 2D by using a Gaussian point-spread function model and Poisson noise to emulate diffraction-limited optics and photon statistics. For each reconstruction, the images of $\textrm{SNR}=2$, $3$, $4$, $5$ (Fig.~\ref{ch2_fig14}) were generated. The simulated images of neurons are obtained this way, each with the exactly known location of the termination and junction point, extrapolated from the source SWC file.
\begin{figure}
	\centering
		\includegraphics[width=0.9\textwidth,height=0.9\textheight,keepaspectratio]{det_nmorpho}
			\caption{Example detections for simulated neuron images at $\textrm{SNR}=4$. The showcased images are contrast enhanced and show the detected terminations (yellow circles) and junctions (red circles) as overlays with fixed radius for better visibility. The value of $\textrm{F}_\textrm{BOTH}$ in these examples is (a) 0.69, (b) 0.85, (c) 0.85, (d) 0.77, (e) 0.75, (f) 0.68, (g) 0.86.}
	\label{ch2_fig16}
\end{figure}
The evaluation results (Fig.~\ref{ch2_fig15}) confirm the conclusion from the experiments on triplets that the method performs well for $\textrm{SNR} \geq 4$ and is somewhat better in detecting terminations than detecting junctions. For $\textrm{SNR} = 4$ the performance for junction detection is $\textrm{F}_{\textrm{JUN}} \approx 0.85$ while for termination detection $\textrm{F}_{\textrm{END}} \approx 0.95$. The higher performance for termination detection may be explained by the fact that the underlying image structure is usually less ambiguous (a single line-like structure surrounded by darker background) than in the case of junctions (multiple line-like structures that are possibly very close to each other). Example detection results are shown in Fig.~\ref{ch2_fig16}.
\begin{figure}
	\centering
	\includegraphics[width=0.4\columnwidth]{overview_real}
	\caption{Performance of the method in detecting terminations and junctions in 30 real fluorescence microscopy images of neurons. The distributions of the $\textrm{F}_\textrm{END}$, $\textrm{F}_\textrm{JUN}$, and $\textrm{F}_\textrm{BOTH}$ values for all images together are shown as box plots.}
	\label{ch2_fig17}
\end{figure}
\subsection{Evaluation on Real Neuron Images}
\label{subsec:experiments-real}
As the ultimate test case, the method was also evaluated on real fluorescence microscopy images of neurons from a published study \cite{steiner2002overexpression}. A total of 30 representative images were taken and expert manual annotations of the critical points were obtained to serve as the gold standard in this experiment. Naturally, real images are generally more challenging than simulated images, as they contain more ambiguities due to labeling and imaging imperfections, and thus the lower performance was expected. Since in this case there is no control over the SNR in the images, the detection results are reported for all images together. The evaluation results (Fig.~\ref{ch2_fig17}) indicate that the median performance in detecting critical points is $\textrm{F}_{\textrm{JUN}}=0.81$ for junctions and $\textrm{F}_{\textrm{END}}=0.73$ for terminations while $\textrm{F}_{\textrm{BOTH}}=0.76$. As expected, these numbers are lower than those of the simulated neuron images. Surprisingly, the junction detection performance with the real images is better than the termination detection which had not been the case with the synthetic neurons. A possible explanation for this could be that in the simulated images a constant intensity was used for the neuron branches, as a result of which terminations are as bright as junctions but much less ambiguous due to a clear background, while in the real images the terminations are usually much less clear due to labeling imperfections and the fact that the branch tips tend to be thinner and thus less bright than the junctions. This illustrates the limitations of the simulations. Example detection results are shown in the Fig.~\ref{ch2_fig18}.
\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth,height=0.7\textheight,keepaspectratio]{det_real}
	\caption{Example detections for four real neuron images. The detected terminations (yellow circles) and junctions (red circles) are shown as overlays with fixed radius for better visibility. The value of $\textrm{F}_\textrm{BOTH}$ in these examples is (a) 0.82, (b) 0.78, (c) 0.68, (d) 0.65.}
	\label{ch2_fig18}
\end{figure}
\subsection{Comparison With Other Methods}
\label{subsec:comparison}
Finally the performance comparison of the Neuron Pinpointer method against other methods is conduct. In lack of availability of other methods explicitly designed to detect and classify critical points in neuron images before reconstruction, two existing software tools relevant in this context were considered and their implicit detection capabilities compared with the presented explicit method. If NP indicates better performance, this would indicate that the existing methods may be improved by exploiting the output of the method.
\begin{figure}
	\centering
	\begin{tabular}{c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}}
	\includegraphics[width=0.3\columnwidth]{compareJUN_all}
	\includegraphics[width=0.3\columnwidth]{compareEND_all}
	\includegraphics[width=0.3\columnwidth]{compareBOTH_all}
	\end{tabular}
	\caption{Critical-point detection performance of the introduced method (NP) compared to two other methods (AS and APP2). The median values of $\textrm{F}_{\textrm{JUN}}$ (left plot) are 0.81 (NP), 0.65 (AS), and 0.47 (APP2). The median values of $\textrm{F}_{\textrm{END}}$ (middle plot) are 0.73 (NP), 0.28 (AS), and 0.21 (APP2). Finally, the median values of $\textrm{F}_{\textrm{BOTH}}$ (right plot) are 0.76 (NP), 0.35 (AS), and 0.29 (APP2).}
	\label{ch2_fig19}
\end{figure}
The first tool, AnalyzeSkeleton (AS)\footnote{available from http://fiji.sc/AnalyzeSkeleton} \cite{arganda20103d}, is an ImageJ plugin for finding and counting all end-points and junctions in a skeleton image. To obtain skeleton images of the neuron images used for this study, the related skeletonization method\footnote{http://fiji.sc/Skeletonize3D} inspired by an advanced thinning algorithm \cite{lee1994building} was used. The input for the latter is a binary image obtained by segmentation based on smoothing (to reduce noise) and thresholding. A range of smoothing scales is considered in the experiments and manually selected thresholds as well as automatically determined thresholds using the following algorithms from ImageJ: Intermodes, Li, MaxEntropy, RenyiEntropy, Moments, Otsu, Triangle, and Yen. All of these were tried in combination with the AS method and the highest F-scores were used.

The second tool, All-Path-Prunning (APP2) \cite{xiao2013app2}, is a plugin for Vaa3D\footnote{available from http://www.vaa3d.org/} \cite{peng2010v3d, peng2014extensible}. It was not designed specifically for a priori critical-point detection but for fully automatic neuron reconstruction. Nevertheless, in producing a tree representation of a neuron, the reconstruction algorithm must somehow identify the branch end-points and junctions, and for aforementioned experiments it is straightforward to retrieve them from the SWC output files. In principle, any neuron reconstruction method is also implicitly a critical-point detection method, and its performance could be quantified by comparing the output tree nodes with the reference data. The interesting question is whether an explicit detector such as NP outperforms the implicit detection carried out in a tool such as APP2. The user parameters of the tool were manually adjusted to get optimal performance in presented experiments.

A comparison of the F-scores of NP, AS, and APP2 for the 30 real neuron images used throughout the experiments is presented in Fig.~\ref{ch2_fig19}. The plots indicate that the detection rates of the NP method are substantially higher than those of AS and APP2. The difference is especially noticeable for the termination points. More specifically, the difference between $\textrm{F}_{\textrm{END}}$ and $\textrm{F}_{\textrm{JUN}}$ is relatively small for NP, but much larger for both AS and APP2. This indicates a clear advantage of using presented explicit and integrated approach for detecting critical points, as accurate neuron reconstruction requires accurate detection of both junctions and terminations. However, with the current implementation, this advantage does come at a cost: timing of the three methods on a standard PC (with Intel Core i7-2630QM 2GHz CPU and 6 GB total RAM) revealed that with the used images of $10^5$ to $10^6$ pixels in size, NP took about 40 seconds per image on average, while both AS and APP2 took only about 1.5 seconds per image. Fortunately, since virtually all the computation time of the presented method is spent in the directional filtering step, which is highly parallelizable, this cost can be reduced to any desired level by employing many-core hardware (such as GPUs).
\section{Conclusions} 
\label{sec:conclusions}
A novel method for solving the important problem of detecting and characterizing critical points in the tree-like structures in neuron microscopy images is presented. Based on directional filtering and feature extraction in combination with a two-stage fuzzy-logic based reasoning system, it provides an integrated framework for the simultaneous identification of both terminations and junctions. From the experimentation on simulated as well as real fluorescence microscopy images, it is possible to conclude that the method achieves substantially higher detection rates than the ones that can be inferred from existing neuron reconstruction methods. This is true for both junction points and termination points, but especially for the latter, which are of key importance in obtaining faithful reconstructions. Altogether, the obtained results suggest that NP may provide important clues to improve the performance of reconstruction methods. Actual integration of the detection method with existing tracing methods is a potential future direction, as the ultimate aim is further utilization, especially from the context of the neuron tracing, as well as adjusting the processing to the 3D imagery. Although the main focus in this work has been the neuron analysis, introduced method may be potentially useful for other applications involving tree-like image structures, such as blood vessel or bronchial tree analysis. Such applications, however, would require further research. For this purpose it may be helpful to increase the robustness of the detection method to larger branch diameter ratios than the ones tested in this paper. This could be done, for example, by using multiscale filtering approaches, or by selective morphological thinning (or thickening). The software implementation of the presented method is available as an ImageJ plugin \footnote{available from https://bitbucket.org/miroslavradojevic/npinpoint}.